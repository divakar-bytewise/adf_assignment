{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40801339-8485-4e08-9847-447731c90298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_base_path = \"abfss://bronze@adfassignment07.dfs.core.windows.net/sales-view\"\n",
    "silver_base_path = \"abfss://silver@adfassignment07.dfs.core.windows.net/sales-view\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc5150ad-ca91-4afc-b00c-f2e162913a6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/appujack799@gmail.com/azure_data_factory_assignment/src/bronze_to_silver/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db943a16-99ac-4738-845c-f3445ad8587e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, when, to_date\n",
    "import re\n",
    "\n",
    "\n",
    "df_customer = spark.read.option(\"header\", True).format(\"csv\").load(f\"{bronze_base_path}/customer/\")\n",
    "display(df_customer)\n",
    "\n",
    "df_customer = clean_and_snake_case_columns(df_customer)\n",
    "df_customer.printSchema()\n",
    "display(df_customer)\n",
    "\n",
    "df_customer = df_customer.withColumn(\"first_name\", split(col(\"name\"), \" \").getItem(0)) \\\n",
    "                         .withColumn(\"last_name\", split(col(\"name\"), \" \").getItem(1))\n",
    "\n",
    "\n",
    "df_customer = df_customer.withColumn(\"domain\", split(col(\"email_id\"), \"@\").getItem(1)) \\\n",
    "                         .withColumn(\"domain\", split(col(\"domain\"), r\"\\.\").getItem(0))\n",
    "\n",
    "\n",
    "df_customer = df_customer.withColumn(\"gender\", when(col(\"gender\") == \"male\", \"M\")\n",
    "                                               .when(col(\"gender\") == \"female\", \"F\")\n",
    "                                               .otherwise(\"O\"))\n",
    "\n",
    "df_customer = df_customer.withColumn(\"joining_date_split\", split(col(\"joining_date\"), \" \")) \\\n",
    "                         .withColumn(\"date\", to_date(col(\"joining_date_split\").getItem(0), \"MM-dd-yyyy\")) \\\n",
    "                         .withColumn(\"time\", col(\"joining_date_split\").getItem(1)) \\\n",
    "                         .drop(\"joining_date_split\")\n",
    "\n",
    "df_customer = df_customer.withColumn(\"spent_numeric\", col(\"spent\").cast(\"double\")) \\\n",
    "                         .withColumn(\"expenditure_status\", \n",
    "                                     when(col(\"spent_numeric\") < 200, \"MINIMUM\")\n",
    "                                     .otherwise(\"MAXIMUM\")) \\\n",
    "                         .drop(\"spent_numeric\")\n",
    "\n",
    "display(df_customer)\n",
    "\n",
    "df_customer.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/customer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac023c82-b150-4a04-8dd6-19dcbbf98874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_product = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/product\")\n",
    "display(df_product)\n",
    "\n",
    "df_product = clean_and_snake_case_columns(df_product)\n",
    "\n",
    "df_product = df_product.withColumn(\"sub_category\", \n",
    "    when(col(\"category_id\") == 1, \"phone\")\n",
    "   .when(col(\"category_id\") == 2, \"laptop\")\n",
    "   .when(col(\"category_id\") == 3, \"playstation\")\n",
    "   .when(col(\"category_id\") == 4, \"e-device\")\n",
    ")\n",
    "\n",
    "display(df_product)\n",
    "\n",
    "df_product.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", True) \\\n",
    "    .save(f\"{silver_base_path}/product\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b329288f-759d-4219-bc70-62442f1cdf7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, regexp_extract, to_date, when\n",
    "\n",
    "df_store = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/store\")\n",
    "df_store.display()\n",
    "\n",
    "df_store = clean_and_snake_case_columns(df_store)\n",
    "\n",
    "df_store = df_store.withColumn(\n",
    "    \"store_category\",\n",
    "    split(split(col(\"email_address\"), \"@\").getItem(1), r\"\\.\").getItem(0)\n",
    ")\n",
    "\n",
    "date_pattern = r\"^\\d{2}-\\d{2}-\\d{4}$\"\n",
    "\n",
    "df_store = df_store.withColumn(\n",
    "    \"created_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"created_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"created_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_store = df_store.withColumn(\n",
    "    \"updated_at\",\n",
    "    when(\n",
    "        regexp_extract(col(\"updated_at\"), date_pattern, 0) != \"\",\n",
    "        to_date(col(\"updated_at\"), \"dd-MM-yyyy\")\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "df_store.display()\n",
    "\n",
    "df_store.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"{silver_base_path}/store\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4a2a7f0-b33d-4d51-b392-c45b704e7509",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sales : Minimal transformations applied and moved from bronze to silver container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8b260b0-626e-42ac-bf4d-cba5344ce4a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "df_sales = spark.read.option(\"header\", True).csv(f\"{bronze_base_path}/sales\")\n",
    "display(df_sales)\n",
    "\n",
    "df_sales = clean_and_snake_case_columns(df_sales)\n",
    "display(df_sales)\n",
    "\n",
    "\n",
    "df_sales = df_sales.withColumn(\n",
    "    \"order_date\", to_timestamp(col(\"order_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ").withColumn(\n",
    "    \"ship_date\", to_timestamp(col(\"ship_date\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\")\n",
    ")\n",
    "\n",
    "\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\").option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(f\"{silver_base_path}/customer_sales\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "drivers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
